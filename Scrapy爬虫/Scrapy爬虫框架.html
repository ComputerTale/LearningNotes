<html>
<head>
  <title>Evernote Export</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/601535 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="816"/>

<div>
<span><div><div><div><div><div><div>安装Scrapy框架：</div><ol><li><div>安装命令'pip install scrapy'</div></li><li><div>在windows下，还需要安装'pypiwin32'</div></li><li><div>在Linux下，还需要安装第三方库 </div></li></ol><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div><span style="font-size: 9pt; color: rgb(51, 51, 51); font-family: Monaco;">sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev </span></div></div><hr/><div>Scrapy基本概念</div><div>Scrapy主要应用于大量的静态网页的爬取</div><div><img src="Scrapy爬虫框架_files/Image.png" type="image/png" data-filename="Image.png"/></div><div><img src="Scrapy爬虫框架_files/Image [1].png" type="image/png" data-filename="Image.png"/></div><div>Scrapy Engine(引擎)：负责Spider、ItemPipeline、Downloader、Sheduler中间的通讯，信号、数据传递等</div><div><br/></div><div>Scheduler（调度器）：它负责接收引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。</div><div><br/></div><div>Downloader（下载器）：负责下载Scrapy Engine（引擎）发送的Request请求，并将其获取到的Response交还给Scrapy Engine（引擎），由引擎交给Spider来处理（Scrapy下载器是建立在twisted这个高效的异步模型上的)</div><div><br/></div><div>Spider（爬虫）：它负责处理所有Response，从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入</div><div>Scheduler（调度器）</div><div><br/></div><div>Item Pipeline（管道）：它负责处理Spider中获取到的Item，并进行后期处理（详细分析、过滤、存储等）的地方</div><div><br/></div><div>Downloader Middlewares（下载中间件）：可以自定义扩展下载功能的组件，主要是处理Scrapy引擎与下载器之间的请求及响应。例如：设置代理ip，请求头。必要</div><div><br/></div><div>Spider Middlewares（爬虫中间件）：自定义扩展和操作引擎和Spider中间通信的功能组件，主要是Spider的响应输入和请求输出。一般不需要</div><div><br/></div><div>整个流程：</div><div><img src="Scrapy爬虫框架_files/Image [2].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div>构建Scrapy爬虫步骤：</div><div><img src="Scrapy爬虫框架_files/Image [3].png" type="image/png" data-filename="Image.png"/></div><hr/><div>构建项目：</div><div>scrapy startproject 项目名</div><div>创建普通爬虫文件</div><div>scrapy genspider 爬虫名 &quot;域名&quot;，爬虫名不能和域名一样</div></div><hr/><div>项目目录</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>scrapy.cfg  配置文件 无需修改</div><div><span style="font-size: 9pt; color: rgb(51, 51, 51); font-family: Monaco;">__init__.py      初始化文件，在这里导入需要的包</span></div><div><span style="font-size: 9pt; color: rgb(51, 51, 51); font-family: Monaco;">items.py        定义需要爬取的字段</span></div><div><span style="font-size: 9pt; color: rgb(51, 51, 51); font-family: Monaco;">middlewares.py   中间件</span></div><div>pipelines.py   处理item数据</div><div>settings.py    设置文件</div><div>spiders/      爬虫目录</div><div><span style="font-size: 9pt; color: rgb(51, 51, 51); font-family: Monaco;">spiders/__init__.py  爬虫默认输出路径，不能删除</span></div></div><div><img src="Scrapy爬虫框架_files/Image [4].png" type="image/png" data-filename="Image.png"/></div><div>Item</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div># -*- coding: utf-8 -*-</div><div># 需要储存的数据</div><div># Define here the models for your scraped items</div><div>#</div><div># See documentation in:</div><div># https://doc.scrapy.org/en/latest/topics/items.html</div><div><br/></div><div>import scrapy</div><div><br/></div><div>class Scrapydemo1Item(scrapy.Item):</div><div>    # define the fields for your item here like:</div><div>    name = scrapy.Field()</div><div>    title = scrapy.Field()</div><div>    info = scrapy.Field()</div><div>    pass</div></div><div>Pipeline</div><div><img src="Scrapy爬虫框架_files/Image [5].png" type="image/png" data-filename="Image.png"/></div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div># -*- coding: utf-8 -*-</div><div># 在这里处理每个item</div><div># Define your item pipelines here</div><div>#</div><div># Don't forget to add your pipeline to the ITEM_PIPELINES setting</div><div># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</div><div><br/></div><div>#第一种方式，这种方式直接存储结果，内存占用资源较少，不过写入的文件为一条条json字典，读取时每读取一行作为一个Json对象</div><div>import json</div><div><br/></div><div>class Scrapydemo1Pipeline(object):</div><div>    # 可选,只在第一个item过来时执行</div><div>    def __init__(self):</div><div>        # 声明一个管道文件</div><div>        self.f = open(&quot;demo_pipeline.json&quot;, &quot;wb&quot;)</div><div>    def open_spider(self,spider):</div><div>        print('爬虫开始...')</div><div>    # 引擎传入spider返回的item</div><div>    def process_item(self, item, spider):</div><div>        # json.dumps 序列化时对中文默认使用的ascii编码，需要取消</div><div>        content = json.dumps(dict(item),ensure_ascii=False)+',\n'</div><div>        self.f.write(content)</div><div>        return item</div><div>    # 可选，爬虫结束时执行</div><div>    def close_spider(self,spider):</div><div>        self.f.close()</div><div>        print('爬虫结束...')</div><div><br/></div><div><br/></div><div>#第二种方式，保存为完整的Json格式，不过这种方法先在内存中开辟控件保存，然后一次写入，占用大量内存资源</div><div># from scrapy.exporters import JsonItemExporter</div><div>#</div><div># class Scrapydemo1Pipeline(object):</div><div>#     # 可选,只在第一个item过来时执行</div><div>#     def __init__(self):</div><div>#         # 声明一个管道文件，wb表示以二进制方式打开,exproter是以二进制存入</div><div>#         self.f = open(&quot;demo_pipeline.json&quot;, &quot;wb&quot;)</div><div>#         self.exporter = JsonItemExporter(self.f, ensure_ascii=False, encoding='utf-8')</div><div>#         self.exporter.start_exporting()</div><div>#</div><div>#     def open_spider(self, spider):</div><div>#         print('爬虫开始...')</div><div>#</div><div>#     # 引擎传入spider返回的item</div><div>#     def process_item(self, item, spider):</div><div>#         self.exporter.export_item(item)</div><div>#         return item</div><div>#</div><div>#     # 可选，爬虫结束时执行</div><div>#     def close_spider(self, spider):</div><div>#         self.exporter.finish_exporting()</div><div>#         self.f.close()</div><div>#         print('爬虫结束...')</div><div><br/></div><div>第三种方式，坏处是每一行是一个json格式</div><div>from scrapy.exporters import JsonLinesItemExporter</div><div><br/></div><div>class BaidunewsPipeline(object):</div><div>    def __init__(self):</div><div>        self.f = open('military.json','ab+')</div><div>        self.exporter = JsonLinesItemExporter(self.f,ensure_ascii = False,encoding='utf-8')</div><div>        self.exporter.start_exporting()</div><div>    def open_spider(self,spider):</div><div>        print('爬虫开始')</div><div>    def process_item(self, item, spider):</div><div>        self.exporter.export_item(item)</div><div>        return item</div><div>    def close_spider(self,spider):</div><div>        self.exporter.finish_exporting()</div><div>        self.f.close()</div><div>        print('爬虫结束')</div></div><div>Spider中处理多次url请求</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div># -*- coding: utf-8 -*-</div><div>import scrapy</div><div># 导入item对象</div><div>from ScrapyDemo1.items import Scrapydemo1Item</div><div><br/></div><div><br/></div><div># 基于scrapy.Spider类</div><div>class DemoSpider(scrapy.Spider):</div><div>    #必要，重写Spider类中的name</div><div>    name = 'demo'</div><div>    # 等价于</div><div>    # def __init__(self):</div><div>    #     self.name = 'demo'</div><div>    # 允许爬虫爬取的域的范围，不必要</div><div>    allowed_domains = ['www.itcast.cn']</div><div>    # 爬虫开始时从列表中的url开始，将url放入调度器</div><div>    # 这里的urls只要是一个迭代对象就可以，列表、元组</div><div>    # 使用元组的时候记得加上(,)</div><div>    # 重写自Spider类的start_url</div><div>    start_urls = ['http://www.itcast.cn/channel/teacher.shtml']</div><div>    # 如果有多个请求，对于每一个请求都调用一个parse方法</div><div>    def parse(self, response):</div><div>        node_list = response.xpath(&quot;//div[@class='li_txt']&quot;)</div><div>        # items =[]</div><div>        for node in node_list:</div><div>            # 创建item字段对象，用来存储对象</div><div>            item = Scrapydemo1Item()</div><div>            # xpath()返回的是一个xpath对象</div><div>            # 使用extract()将xpath对象转换为Unicode字符串</div><div>            # 返回的是一个列表</div><div>            name = node.xpath(&quot;./h3/text()&quot;).extract()</div><div>            title = node.xpath(&quot;./h4/text()&quot;).extract()</div><div>            info = node.xpath(&quot;./p/text()&quot;).extract()</div><div><br/></div><div>            item['name'] = name[0]</div><div>            item['title'] = title[0]</div><div>            item['info'] = info[0]</div><div>            # yield表示一个生成器</div><div>            # 调用parse时会继续该循环</div><div>            # 将获取的数据交给pipeline</div><div>            # 好处避免使用[]来存储临时数据，造成内存占用过大</div><div>            yield item</div><div>            # items.append(item)</div><div>        # 返回给引擎</div><div>        # return items</div></div><div><img src="Scrapy爬虫框架_files/Image [6].png" type="image/png" data-filename="Image.png"/></div><hr/><div>在Pycharm中，需要单独写一个启动scrapy爬虫的文件，然后执行该文件</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>from scrapy import cmdline</div><div><br/></div><div>cmdline.execute('scrpay crawl demo -s LOG_FILE=all.log'.split())</div></div><div>运行结果不直接在窗口输出，以文件形式保存</div><div><img src="Scrapy爬虫框架_files/Image [7].png" type="image/png" data-filename="Image.png"/></div></div><hr/><div style="box-sizing: border-box; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; font-size: medium; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="box-sizing: border-box; -webkit-tap-highlight-color: transparent; font-size: medium; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(0, 0, 0); font-family: 微软雅黑; font-variant-caps: normal; font-variant-ligatures: normal;">创建CrawlSpider</span></div><div style="box-sizing: border-box; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; font-size: medium; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(77, 77, 77); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取</span></div><div style="box-sizing: border-box; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 8px; font-size: 12px; border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15); letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><div style="box-sizing: border-box; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px;"><span style="box-sizing: border-box; -webkit-tap-highlight-color: transparent; font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-variant-caps: normal; font-variant-ligatures: normal;">scrapy genspider -t crawl [爬虫名称] [域名]</span></div></div><div style="box-sizing: border-box; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; font-size: medium; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: medium; color: rgb(0, 0, 0); font-family: 微软雅黑; font-variant-caps: normal; font-variant-ligatures: normal;"><img src="Scrapy爬虫框架_files/Image [8].png" type="image/png" data-filename="Image.png" style="box-sizing: initial; -webkit-tap-highlight-color: transparent; cursor: default; position: relative; display: inline-block; padding: 1px; margin: 0px; vertical-align: text-bottom; max-width: 100%; height: auto;"/></span></div><div style="box-sizing: border-box; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; font-size: medium; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: medium; color: rgb(0, 0, 0); font-family: 微软雅黑; font-variant-caps: normal; font-variant-ligatures: normal;"><img src="Scrapy爬虫框架_files/Image [9].png" type="image/png" data-filename="Image.png" style="box-sizing: initial; -webkit-tap-highlight-color: transparent; cursor: default; position: relative; display: inline-block; padding: 1px; margin: 0px; vertical-align: text-bottom; max-width: 100%; height: auto;"/></span></div><div style="box-sizing: border-box; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; font-size: medium; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="box-sizing: border-box; -webkit-tap-highlight-color: transparent; font-size: medium; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(0, 0, 0); font-family: 微软雅黑; font-variant-caps: normal; font-variant-ligatures: normal;">follow可以理解为如果URL的页面中出现符合规则的URL，要递归爬取</span></div><div style="box-sizing: border-box; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; font-size: medium; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><br/></div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div># -*- coding: utf-8 -*-</div><div>import scrapy</div><div>from scrapy.linkextractors import LinkExtractor</div><div>from scrapy.spiders import CrawlSpider, Rule</div><div>from ScrayDemo2.items import Scraydemo2Item</div><div><br/></div><div>class WxappSpiderSpider(CrawlSpider):</div><div>    name = 'wxapp_spider'</div><div>    allowed_domains = ['wxapp-union.com']</div><div>    start_urls = ['http://www.wxapp-union.com/portal.php?mod=list&amp;catid=1&amp;page=1']</div><div><br/></div><div>    rules = (</div><div>        # 链接提取器是以正则表达式规则完成</div><div>        # 第一个链接提取器不需要callback，因为这是专门用来提取链接的</div><div>        Rule(LinkExtractor(allow=r'.+mod=list&amp;catid=1&amp;page=\d'), follow=True),</div><div>        # 第二个链接提取器才是用来提取内容的</div><div>        Rule(LinkExtractor(allow=r'.+article-.+\.html'), callback=&quot;parse_detail&quot;, follow=False)</div><div>    )</div><div><br/></div><div>    # 与BasicSpider不同,这里要自定义parse方法,并且名称不能为parse()</div><div>    # 因为parse()方法以直接重写Spider里面的方法，会冲突</div><div>    def parse_detail(self, response):</div><div>        item = {}</div><div>        title = response.xpath(&quot;//h1[@class='ph']/text()&quot;).get()</div><div>        author_p = response.xpath(&quot;//p[@class='authors']&quot;)</div><div>        author = author_p.xpath(&quot;.//a/text()&quot;).get()</div><div>        pub_time = author_p.xpath(&quot;.//span/text()&quot;).get()</div><div>        # 返回一个列表</div><div>        article_content = response.xpath(&quot;//td[@id='article_content']//text()&quot;).getall()</div><div>        # print(article_content)</div><div>        # ().join(list)：将序列list中的元素以str连接成为一个新的字符串</div><div>        # str.strip()：移除字符串头尾指定的字符(默认为空格或换行)</div><div>        content = &quot;&quot;.join(article_content).strip()</div><div>        item = Scraydemo2Item(title=title, author=author, pub_time=pub_time, content=content)</div><div>        # return item</div></div><hr/><div>Request</div></div><div><img src="Scrapy爬虫框架_files/Image [10].png" type="image/png" data-filename="Image.png"/></div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div><font style="font-size: 14pt;">url:请求对象的url</font></div><div><span style="color: rgb(51, 51, 51); font-family: Monaco;"><font style="font-size: 14pt;">callback:在下载器下载完后的回调函数</font></span></div><div><span style="color: rgb(51, 51, 51); font-family: Monaco;"><font style="font-size: 14pt;">method:请求的方法，默认为GET方法，可以设置其他方法，如果使用POST方法，建议使用 FormRequest</font></span></div><div><span style="color: rgb(51, 51, 51); font-family: Monaco;"><font style="font-size: 14pt;">headers:请求头，对于一些固定的请求头，在settings指定即可。对于那些非固定的，可以在发送请求时指定</font></span></div><div><span style="color: rgb(51, 51, 51); font-family: Monaco;"><font style="font-size: 14pt;">meta:在请求和相应之间传递数据</font></span></div><div><span style="color: rgb(51, 51, 51); font-family: Monaco;"><font style="font-size: 14pt;">encoding:默认utf-8,</font></span></div><div><span style="color: rgb(51, 51, 51); font-family: Monaco;"><font style="font-size: 14pt;">dont_filter:表示不由调度器过滤，在执行多次重复的请求的时候使用，如一些网站有二维码，先得对爬取的网页进行识别二维码处理，然后重新登录该网站，此时dont_filter就得设置为False</font></span></div><div><span style="color: rgb(51, 51, 51); font-family: Monaco;"><font style="font-size: 14pt;">errback:发生错误时执行的函数</font></span></div></div><div>Response</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>meta:从其他请求传过来的meta属性，可以用来保持多个请求之间的数据连接</div><div>encoding:返回当前字符串编码和解码的格式</div><div>text:将返回的数据作为unicode字符串返回</div><div><span style="font-size: 9pt; color: rgb(51, 51, 51); font-family: Monaco;">body:将返回的数据作为bytes字符串返回</span></div><div><span style="font-size: 9pt; color: rgb(51, 51, 51); font-family: Monaco;">xpath:xpath选择器</span></div><div><span style="font-size: 9pt; color: rgb(51, 51, 51); font-family: Monaco;">css:css选择器</span></div></div><div>发送POST请求</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>想要发送POST请求，需要使用Request的子类FormRequest。如果想要在爬虫一开始就发送POST请求，需要在爬虫中重写start_requests方法，并且不再调用start_urls里的url</div><div><br/></div><div>url = &quot;http://www.renren.com/PLogin.do&quot;</div><div>#data中所有值都是字符串，不能使数字和boolean值</div><div>data = {&quot;email&quot;:&quot;...qq.com&quot;,&quot;password&quot;:&quot;...&quot;}</div><div># 以post方式发送表单请求</div><div>request = scrapy.FormRequest(url,formdata=data,callback=self.parse())</div><div>yield request</div></div><hr/><div>爬虫中间件和下载中间件</div><div>process_request(self,request,spider)：处于引擎到下载器之间</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div><font face="Monaco">参数</font></div><div>request：发送请求的request对象</div><div>spider：发送请求的spider对象</div><div>返回值：</div><div><font face="Monaco">返回None：Scrapy将继续处理该request，执行中间件中的响应方法，直到合适的下载器处理函数被调用</font></div><div><font face="Monaco">返回Response对象：Scrapy将不会调用任何其他process_request方法，将直接返回这个对象，以激活的中间件的process_reponse()方法会在每个response返回时调用</font></div><div><font face="Monaco">返回Request对象：不再使用之前的request对象去下载数据，而是根据现在返回的request对象返回数据。</font></div><div><font face="Monaco">异常：调用process_exception方法</font></div></div><div>process_response(self,request,response,spider)：处于下载器和引擎之间</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>参数：</div><div>request:request对象</div><div>reponse:被处理的response对象</div><div>spider：</div><div>返回值：</div><div>Response对象：将这个reponse对象传给其他中间件，最终传给爬虫</div><div><span style="font-size: 9pt; color: rgb(51, 51, 51); font-family: Monaco;">Request对象：将这个request对象传给下载器</span></div></div><hr/><div>设置随机请求头</div></div><div>获取你当前的请求信息</div><div><a href="http://httpbin.org/">http://httpbin.org/</a></div><div>请求头查找网站</div><div><a href="http://useragentstring.com/pages/useragentstring.php?typ=Browser">http://useragentstring.com/pages/useragentstring.php?typ=Browser</a></div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div># -*- coding: utf-8 -*-</div><div>import scrapy</div><div>import json</div><div><br/></div><div>class HttpbinSpider(scrapy.Spider):</div><div>    name = 'httpbin'</div><div>    allowed_domains = ['httpbin.org']</div><div>    start_urls = ['http://httpbin.org/user-agent']</div><div><br/></div><div>    def parse(self, response):</div><div>        user_agent = json.loads(response.text)['user-agent']</div><div>        print(user_agent)</div><div>        print('='*20)</div><div>        yield scrapy.Request(self.start_urls[0],dont_filter=True )</div></div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>class MiddlewareDemoDownloaderMiddleware(object):</div><div>    # Not all methods need to be defined. If a method is not defined,</div><div>    # scrapy acts as if the downloader middleware does not modify the</div><div>    # passed objects.</div><div>    USER_AGENTS = [</div><div>        'Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Acoo Browser 1.98.744; .NET CLR 3.5.30729)',</div><div>        'Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Acoo Browser 1.98.744; .NET CLR 3.5.30729)',</div><div>        'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Acoo Browser; GTB5; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; InfoPath.1; .NET CLR 3.5.30729; .NET CLR 3.0.30618)',</div><div>        'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; SV1; Acoo Browser; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; Avant Browser)',</div><div>        'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)',</div><div>        'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; GTB5; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; Maxthon; InfoPath.1; .NET CLR 3.5.30729; .NET CLR 3.0.30618)',</div><div>        'Mozilla/4.0 (compatible; Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Acoo Browser 1.98.744; .NET CLR 3.5.30729); Windows NT 5.1; Trident/4.0)',</div><div>        'Mozilla/4.0 (compatible; Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; GTB6; Acoo Browser; .NET CLR 1.1.4322; .NET CLR 2.0.50727); Windows NT 5.1; Trident/4.0; Maxthon; .NET CLR 2.0.50727; .NET CLR 1.1.4322; InfoPath.2)',</div><div>        'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Acoo Browser; GTB6; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; InfoPath.1; .NET CLR 3.5.30729; .NET CLR 3.0.30618)',</div><div>        'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Acoo Browser; GTB5; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; InfoPath.1; .NET CLR 3.5.30729; .NET CLR 3.0.30618)',</div><div>        'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; GTB6; Acoo Browser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',</div><div>        'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Trident/4.0; Acoo Browser; GTB5; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; InfoPath.1; .NET CLR 3.5.30729; .NET CLR 3.0.30618)',</div><div>        'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)',</div><div>        'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; GTB5; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)',</div><div>        'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; GTB5; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; InfoPath.1; .NET CLR 3.5.30729; .NET CLR 3.0.30618)',</div><div>        'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Acoo Browser; InfoPath.2; .NET CLR 2.0.50727; Alexa Toolbar)',</div><div>        'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Acoo Browser; .NET CLR 2.0.50727; .NET CLR 1.1.4322)',</div><div>        'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Acoo Browser; .NET CLR 1.0.3705; .NET CLR 1.1.4322; .NET CLR 2.0.50727; FDM; .NET CLR 3.0.04506.30; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022; InfoPath.2)',</div><div>        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; Acoo Browser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)'</div><div>    ]</div><div><br/></div><div>def process_request(self, request, spider):</div><div>    user_agent = random.choice(self.USER_AGENTS)</div><div>    request.headers['User-Agent'] = user_agent</div><div>    return None</div></div><hr/><div>设置代理ip</div><div><br/></div></div><div><br/></div></span>
</div></body></html> 